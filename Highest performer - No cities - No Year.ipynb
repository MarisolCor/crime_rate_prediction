{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8d4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210c26d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>City</th>\n",
       "      <th>Population 25 years and over - Less than 9th grade</th>\n",
       "      <th>Percent Population 25 years and over - Less than 9th grade</th>\n",
       "      <th>Population 25 years and over - 9th to12th (No Diploma)</th>\n",
       "      <th>Percent Population 25 years and over - 9th to12th (No Diploma)</th>\n",
       "      <th>Population 25 years and over - High School Graduate (and equivalent)</th>\n",
       "      <th>Percent Population 25 years and over - High School Graduate (and equivalent)</th>\n",
       "      <th>Population 25 years and over - Some college, no degree</th>\n",
       "      <th>Percent Population 25 years and over - Some college, no degree</th>\n",
       "      <th>...</th>\n",
       "      <th>Percent Home Occupied</th>\n",
       "      <th>Percent Renter Occupied</th>\n",
       "      <th>Percent 25+</th>\n",
       "      <th>Percent 14-</th>\n",
       "      <th>Percent 15 to 24</th>\n",
       "      <th>Percent Uneducated</th>\n",
       "      <th>Percent Higher Education</th>\n",
       "      <th>Sum Uneducated</th>\n",
       "      <th>Sum Higher Education</th>\n",
       "      <th>City_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2752</td>\n",
       "      <td>5.50</td>\n",
       "      <td>2234</td>\n",
       "      <td>4.50</td>\n",
       "      <td>8546</td>\n",
       "      <td>17.00</td>\n",
       "      <td>10009</td>\n",
       "      <td>19.90</td>\n",
       "      <td>...</td>\n",
       "      <td>46.920853</td>\n",
       "      <td>53.079147</td>\n",
       "      <td>65.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>10.00</td>\n",
       "      <td>53.10</td>\n",
       "      <td>4986</td>\n",
       "      <td>26639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>Alhambra</td>\n",
       "      <td>6236</td>\n",
       "      <td>10.10</td>\n",
       "      <td>4575</td>\n",
       "      <td>7.40</td>\n",
       "      <td>13358</td>\n",
       "      <td>21.70</td>\n",
       "      <td>12091</td>\n",
       "      <td>19.60</td>\n",
       "      <td>...</td>\n",
       "      <td>38.053381</td>\n",
       "      <td>61.946619</td>\n",
       "      <td>71.1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>13.1</td>\n",
       "      <td>17.50</td>\n",
       "      <td>41.10</td>\n",
       "      <td>10811</td>\n",
       "      <td>25307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>32561</td>\n",
       "      <td>15.50</td>\n",
       "      <td>22329</td>\n",
       "      <td>10.70</td>\n",
       "      <td>50982</td>\n",
       "      <td>24.30</td>\n",
       "      <td>39407</td>\n",
       "      <td>18.80</td>\n",
       "      <td>...</td>\n",
       "      <td>48.388510</td>\n",
       "      <td>51.611490</td>\n",
       "      <td>61.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16.7</td>\n",
       "      <td>26.20</td>\n",
       "      <td>30.70</td>\n",
       "      <td>54890</td>\n",
       "      <td>64362</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>Antioch</td>\n",
       "      <td>4874</td>\n",
       "      <td>7.70</td>\n",
       "      <td>4633</td>\n",
       "      <td>7.30</td>\n",
       "      <td>17092</td>\n",
       "      <td>26.90</td>\n",
       "      <td>20113</td>\n",
       "      <td>31.70</td>\n",
       "      <td>...</td>\n",
       "      <td>62.767194</td>\n",
       "      <td>37.232806</td>\n",
       "      <td>60.7</td>\n",
       "      <td>22.9</td>\n",
       "      <td>15.5</td>\n",
       "      <td>15.00</td>\n",
       "      <td>26.50</td>\n",
       "      <td>9507</td>\n",
       "      <td>16815</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>Apple Valley</td>\n",
       "      <td>639</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3353</td>\n",
       "      <td>7.70</td>\n",
       "      <td>14524</td>\n",
       "      <td>33.40</td>\n",
       "      <td>11690</td>\n",
       "      <td>26.90</td>\n",
       "      <td>...</td>\n",
       "      <td>70.690377</td>\n",
       "      <td>29.309623</td>\n",
       "      <td>60.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>9.20</td>\n",
       "      <td>30.50</td>\n",
       "      <td>3992</td>\n",
       "      <td>13303</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>2019</td>\n",
       "      <td>Vista</td>\n",
       "      <td>10080</td>\n",
       "      <td>15.05</td>\n",
       "      <td>6638</td>\n",
       "      <td>9.91</td>\n",
       "      <td>13245</td>\n",
       "      <td>19.77</td>\n",
       "      <td>16027</td>\n",
       "      <td>23.93</td>\n",
       "      <td>...</td>\n",
       "      <td>45.724859</td>\n",
       "      <td>54.275141</td>\n",
       "      <td>64.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>14.1</td>\n",
       "      <td>24.96</td>\n",
       "      <td>31.34</td>\n",
       "      <td>16718</td>\n",
       "      <td>20990</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>2019</td>\n",
       "      <td>West Covina</td>\n",
       "      <td>5308</td>\n",
       "      <td>7.09</td>\n",
       "      <td>4530</td>\n",
       "      <td>6.05</td>\n",
       "      <td>20919</td>\n",
       "      <td>27.95</td>\n",
       "      <td>16083</td>\n",
       "      <td>21.49</td>\n",
       "      <td>...</td>\n",
       "      <td>57.975360</td>\n",
       "      <td>42.024640</td>\n",
       "      <td>69.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>13.14</td>\n",
       "      <td>37.41</td>\n",
       "      <td>9838</td>\n",
       "      <td>28003</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>2019</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>8307</td>\n",
       "      <td>13.03</td>\n",
       "      <td>6607</td>\n",
       "      <td>10.36</td>\n",
       "      <td>15641</td>\n",
       "      <td>24.53</td>\n",
       "      <td>11273</td>\n",
       "      <td>17.68</td>\n",
       "      <td>...</td>\n",
       "      <td>47.216869</td>\n",
       "      <td>52.783131</td>\n",
       "      <td>68.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>23.39</td>\n",
       "      <td>34.41</td>\n",
       "      <td>14914</td>\n",
       "      <td>21943</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>2019</td>\n",
       "      <td>Whittier</td>\n",
       "      <td>3021</td>\n",
       "      <td>5.44</td>\n",
       "      <td>3921</td>\n",
       "      <td>7.06</td>\n",
       "      <td>16232</td>\n",
       "      <td>29.24</td>\n",
       "      <td>12079</td>\n",
       "      <td>21.76</td>\n",
       "      <td>...</td>\n",
       "      <td>60.928433</td>\n",
       "      <td>39.071567</td>\n",
       "      <td>63.3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>12.50</td>\n",
       "      <td>36.49</td>\n",
       "      <td>6942</td>\n",
       "      <td>20258</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>2019</td>\n",
       "      <td>Yuba City</td>\n",
       "      <td>5344</td>\n",
       "      <td>11.93</td>\n",
       "      <td>4756</td>\n",
       "      <td>10.62</td>\n",
       "      <td>9396</td>\n",
       "      <td>20.97</td>\n",
       "      <td>11112</td>\n",
       "      <td>24.80</td>\n",
       "      <td>...</td>\n",
       "      <td>54.540237</td>\n",
       "      <td>45.459763</td>\n",
       "      <td>64.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.55</td>\n",
       "      <td>31.69</td>\n",
       "      <td>10100</td>\n",
       "      <td>14192</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1165 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year          City  Population 25 years and over - Less than 9th grade  \\\n",
       "0     2010       Alameda                                               2752    \n",
       "1     2010      Alhambra                                               6236    \n",
       "2     2010       Anaheim                                              32561    \n",
       "3     2010       Antioch                                               4874    \n",
       "4     2010  Apple Valley                                                639    \n",
       "...    ...           ...                                                ...    \n",
       "1160  2019         Vista                                              10080    \n",
       "1161  2019   West Covina                                               5308    \n",
       "1162  2019   Westminster                                               8307    \n",
       "1163  2019      Whittier                                               3021    \n",
       "1164  2019     Yuba City                                               5344    \n",
       "\n",
       "      Percent Population 25 years and over - Less than 9th grade  \\\n",
       "0                                                  5.50            \n",
       "1                                                 10.10            \n",
       "2                                                 15.50            \n",
       "3                                                  7.70            \n",
       "4                                                  1.50            \n",
       "...                                                 ...            \n",
       "1160                                              15.05            \n",
       "1161                                               7.09            \n",
       "1162                                              13.03            \n",
       "1163                                               5.44            \n",
       "1164                                              11.93            \n",
       "\n",
       "      Population 25 years and over - 9th to12th (No Diploma)  \\\n",
       "0                                                  2234        \n",
       "1                                                  4575        \n",
       "2                                                 22329        \n",
       "3                                                  4633        \n",
       "4                                                  3353        \n",
       "...                                                 ...        \n",
       "1160                                               6638        \n",
       "1161                                               4530        \n",
       "1162                                               6607        \n",
       "1163                                               3921        \n",
       "1164                                               4756        \n",
       "\n",
       "      Percent Population 25 years and over - 9th to12th (No Diploma)  \\\n",
       "0                                                  4.50                \n",
       "1                                                  7.40                \n",
       "2                                                 10.70                \n",
       "3                                                  7.30                \n",
       "4                                                  7.70                \n",
       "...                                                 ...                \n",
       "1160                                               9.91                \n",
       "1161                                               6.05                \n",
       "1162                                              10.36                \n",
       "1163                                               7.06                \n",
       "1164                                              10.62                \n",
       "\n",
       "      Population 25 years and over - High School Graduate (and equivalent)  \\\n",
       "0                                                  8546                      \n",
       "1                                                 13358                      \n",
       "2                                                 50982                      \n",
       "3                                                 17092                      \n",
       "4                                                 14524                      \n",
       "...                                                 ...                      \n",
       "1160                                              13245                      \n",
       "1161                                              20919                      \n",
       "1162                                              15641                      \n",
       "1163                                              16232                      \n",
       "1164                                               9396                      \n",
       "\n",
       "      Percent Population 25 years and over - High School Graduate (and equivalent)  \\\n",
       "0                                                 17.00                              \n",
       "1                                                 21.70                              \n",
       "2                                                 24.30                              \n",
       "3                                                 26.90                              \n",
       "4                                                 33.40                              \n",
       "...                                                 ...                              \n",
       "1160                                              19.77                              \n",
       "1161                                              27.95                              \n",
       "1162                                              24.53                              \n",
       "1163                                              29.24                              \n",
       "1164                                              20.97                              \n",
       "\n",
       "      Population 25 years and over - Some college, no degree  \\\n",
       "0                                                 10009        \n",
       "1                                                 12091        \n",
       "2                                                 39407        \n",
       "3                                                 20113        \n",
       "4                                                 11690        \n",
       "...                                                 ...        \n",
       "1160                                              16027        \n",
       "1161                                              16083        \n",
       "1162                                              11273        \n",
       "1163                                              12079        \n",
       "1164                                              11112        \n",
       "\n",
       "      Percent Population 25 years and over - Some college, no degree  ...  \\\n",
       "0                                                 19.90               ...   \n",
       "1                                                 19.60               ...   \n",
       "2                                                 18.80               ...   \n",
       "3                                                 31.70               ...   \n",
       "4                                                 26.90               ...   \n",
       "...                                                 ...               ...   \n",
       "1160                                              23.93               ...   \n",
       "1161                                              21.49               ...   \n",
       "1162                                              17.68               ...   \n",
       "1163                                              21.76               ...   \n",
       "1164                                              24.80               ...   \n",
       "\n",
       "      Percent Home Occupied  Percent Renter Occupied  Percent 25+  \\\n",
       "0                 46.920853                53.079147         65.8   \n",
       "1                 38.053381                61.946619         71.1   \n",
       "2                 48.388510                51.611490         61.0   \n",
       "3                 62.767194                37.232806         60.7   \n",
       "4                 70.690377                29.309623         60.4   \n",
       "...                     ...                      ...          ...   \n",
       "1160              45.724859                54.275141         64.4   \n",
       "1161              57.975360                42.024640         69.5   \n",
       "1162              47.216869                52.783131         68.0   \n",
       "1163              60.928433                39.071567         63.3   \n",
       "1164              54.540237                45.459763         64.6   \n",
       "\n",
       "      Percent 14-  Percent 15 to 24  Percent Uneducated  \\\n",
       "0            19.8              12.3               10.00   \n",
       "1            12.9              13.1               17.50   \n",
       "2            21.2              16.7               26.20   \n",
       "3            22.9              15.5               15.00   \n",
       "4            23.4              13.9                9.20   \n",
       "...           ...               ...                 ...   \n",
       "1160         19.9              14.1               24.96   \n",
       "1161         16.9              11.9               13.14   \n",
       "1162         18.2              11.5               23.39   \n",
       "1163         21.0              13.9               12.50   \n",
       "1164         18.2              15.0               22.55   \n",
       "\n",
       "      Percent Higher Education  Sum Uneducated  Sum Higher Education  \\\n",
       "0                        53.10            4986                 26639   \n",
       "1                        41.10           10811                 25307   \n",
       "2                        30.70           54890                 64362   \n",
       "3                        26.50            9507                 16815   \n",
       "4                        30.50            3992                 13303   \n",
       "...                        ...             ...                   ...   \n",
       "1160                     31.34           16718                 20990   \n",
       "1161                     37.41            9838                 28003   \n",
       "1162                     34.41           14914                 21943   \n",
       "1163                     36.49            6942                 20258   \n",
       "1164                     31.69           10100                 14192   \n",
       "\n",
       "      City_encoded  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  \n",
       "...            ...  \n",
       "1160           129  \n",
       "1161           131  \n",
       "1162           132  \n",
       "1163           133  \n",
       "1164           135  \n",
       "\n",
       "[1165 rows x 62 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our input dataset\n",
    "crime_df = pd.read_csv('All_data_cleaned.csv')\n",
    "crime_df.head()\n",
    "\n",
    "# One-hot encode the 'City' column\n",
    "crime_encoded = pd.get_dummies(crime_df, columns=['City'])\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'City' column\n",
    "crime_df['City_encoded'] = label_encoder.fit_transform(crime_df['City'])\n",
    "\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e489afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the seed for random number generation\n",
    "seed_value = 8\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Remove crime and city target from features data\n",
    "X = crime_df.copy()\n",
    "X.drop(columns=[\"Crime_Rate_per_100k\", \n",
    "                \"City\",\n",
    "                \"Violent Crimes Sum\",\n",
    "                \"City_encoded\",\n",
    "                \"Year\"\n",
    "               ], axis=1, inplace=True)\n",
    "y = crime_df['Crime_Rate_per_100k']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dce194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 232762.9062 - val_loss: 213039.2031\n",
      "Epoch 2/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 168957.9062 - val_loss: 100393.9922\n",
      "Epoch 3/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103222.6953 - val_loss: 62571.3125\n",
      "Epoch 4/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79829.3125 - val_loss: 51779.1641\n",
      "Epoch 5/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67942.4688 - val_loss: 44368.9961\n",
      "Epoch 6/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59244.6484 - val_loss: 40240.9219\n",
      "Epoch 7/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52273.3008 - val_loss: 37144.9922\n",
      "Epoch 8/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46682.1719 - val_loss: 34595.2617\n",
      "Epoch 9/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41428.3281 - val_loss: 32360.1270\n",
      "Epoch 10/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37867.3281 - val_loss: 30884.0723\n",
      "Epoch 11/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34541.0898 - val_loss: 29830.7324\n",
      "Epoch 12/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32142.1523 - val_loss: 28586.1094\n",
      "Epoch 13/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29953.6836 - val_loss: 27526.8828\n",
      "Epoch 14/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27642.2402 - val_loss: 26402.3652\n",
      "Epoch 15/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25311.3359 - val_loss: 25455.4629\n",
      "Epoch 16/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23061.3535 - val_loss: 24954.6934\n",
      "Epoch 17/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21091.5781 - val_loss: 24384.5371\n",
      "Epoch 18/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19678.6074 - val_loss: 24033.1484\n",
      "Epoch 19/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18543.4082 - val_loss: 23953.6895\n",
      "Epoch 20/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17706.1348 - val_loss: 23766.4160\n",
      "Epoch 21/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17019.2363 - val_loss: 23699.1836\n",
      "Epoch 22/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16508.3555 - val_loss: 23587.0293\n",
      "Epoch 23/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16053.2783 - val_loss: 23492.7812\n",
      "Epoch 24/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15605.3379 - val_loss: 23401.1543\n",
      "Epoch 25/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15202.4004 - val_loss: 23370.8906\n",
      "Epoch 26/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14839.6631 - val_loss: 23366.2891\n",
      "Epoch 27/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14464.8682 - val_loss: 23349.6113\n",
      "Epoch 28/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14086.8066 - val_loss: 23373.2637\n",
      "Epoch 29/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13713.1396 - val_loss: 23428.0469\n",
      "Epoch 30/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13339.3486 - val_loss: 23488.5195\n",
      "Epoch 31/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12979.0312 - val_loss: 23538.2246\n",
      "Epoch 32/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12608.3135 - val_loss: 23629.3145\n",
      "Epoch 33/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12268.8467 - val_loss: 23657.6016\n",
      "Epoch 34/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11911.0039 - val_loss: 23670.6934\n",
      "Epoch 35/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11592.2715 - val_loss: 23628.7363\n",
      "Epoch 36/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11249.9023 - val_loss: 23655.7559\n",
      "Epoch 37/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10945.3379 - val_loss: 23581.0645\n",
      "Epoch 38/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10604.7910 - val_loss: 23530.9492\n",
      "Epoch 39/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10311.1406 - val_loss: 23398.2227\n",
      "Epoch 40/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9987.9766 - val_loss: 23283.4785\n",
      "Epoch 41/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9683.3389 - val_loss: 23146.0391\n",
      "Epoch 42/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9358.0977 - val_loss: 23045.7695\n",
      "Epoch 43/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9082.3750 - val_loss: 22942.2578\n",
      "Epoch 44/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8799.1357 - val_loss: 22709.8027\n",
      "Epoch 45/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8490.8594 - val_loss: 22694.2461\n",
      "Epoch 46/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8237.5586 - val_loss: 22435.1074\n",
      "Epoch 47/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7935.7578 - val_loss: 22576.3535\n",
      "Epoch 48/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7728.5493 - val_loss: 22378.8789\n",
      "Epoch 49/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7391.7446 - val_loss: 22495.5684\n",
      "Epoch 50/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7195.5186 - val_loss: 22325.0996\n",
      "Epoch 51/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6904.7754 - val_loss: 22409.2969\n",
      "Epoch 52/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6687.9253 - val_loss: 22388.1016\n",
      "Epoch 53/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6414.5923 - val_loss: 22360.2109\n",
      "Epoch 54/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6165.8779 - val_loss: 22598.4062\n",
      "Epoch 55/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5970.3135 - val_loss: 22491.8828\n",
      "Epoch 56/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5687.2534 - val_loss: 22772.4551\n",
      "Epoch 57/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5496.8892 - val_loss: 22696.3613\n",
      "Epoch 58/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5223.2139 - val_loss: 22964.0117\n",
      "Epoch 59/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5040.7861 - val_loss: 23031.1445\n",
      "Epoch 60/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4800.9604 - val_loss: 23015.0840\n",
      "Epoch 61/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4569.2231 - val_loss: 23177.2246\n",
      "Epoch 62/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4381.5264 - val_loss: 23251.2109\n",
      "Epoch 63/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4177.6689 - val_loss: 23338.8203\n",
      "Epoch 64/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3999.3909 - val_loss: 23346.8281\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3803.1008 - val_loss: 23430.6992\n",
      "Epoch 66/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3641.7292 - val_loss: 23470.9531\n",
      "Epoch 67/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3473.2866 - val_loss: 23524.4082\n",
      "Epoch 68/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3324.8127 - val_loss: 23529.2480\n",
      "Epoch 69/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3173.9326 - val_loss: 23562.8105\n",
      "Epoch 70/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3040.6145 - val_loss: 23581.7070\n",
      "Epoch 71/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2912.0134 - val_loss: 23617.3457\n",
      "Epoch 72/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2794.8218 - val_loss: 23646.4336\n",
      "Epoch 73/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2686.1323 - val_loss: 23689.7812\n",
      "Epoch 74/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2584.3579 - val_loss: 23750.9551\n",
      "Epoch 75/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2493.2778 - val_loss: 23827.7539\n",
      "Epoch 76/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2408.9050 - val_loss: 23920.1367\n",
      "Epoch 77/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2333.5625 - val_loss: 24031.1230\n",
      "Epoch 78/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2266.3845 - val_loss: 24159.7715\n",
      "Epoch 79/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2207.9165 - val_loss: 24313.0234\n",
      "Epoch 80/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2160.9229 - val_loss: 24485.9766\n",
      "Epoch 81/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2125.7158 - val_loss: 24672.1680\n",
      "Epoch 82/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2105.9600 - val_loss: 24870.4141\n",
      "Epoch 83/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2106.9653 - val_loss: 25079.3047\n",
      "Epoch 84/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2139.2974 - val_loss: 25298.9297\n",
      "Epoch 85/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2223.9248 - val_loss: 25524.5059\n",
      "Epoch 86/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2399.9822 - val_loss: 25745.6406\n",
      "Epoch 87/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2751.2864 - val_loss: 25938.0547\n",
      "Epoch 88/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3468.7051 - val_loss: 25720.4160\n",
      "Epoch 89/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4921.8306 - val_loss: 27063.2344\n",
      "Epoch 90/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7087.8896 - val_loss: 41219.7188\n",
      "Epoch 91/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8046.9956 - val_loss: 36235.2891\n",
      "Epoch 92/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8572.0703 - val_loss: 31657.5117\n",
      "Epoch 93/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5965.4561 - val_loss: 29770.8828\n",
      "Epoch 94/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3905.7405 - val_loss: 29449.7617\n",
      "Epoch 95/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3373.0095 - val_loss: 30014.3516\n",
      "Epoch 96/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3341.6738 - val_loss: 30240.4473\n",
      "Epoch 97/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3374.8169 - val_loss: 30297.5566\n",
      "Epoch 98/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3283.0840 - val_loss: 30380.2402\n",
      "Epoch 99/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3179.1245 - val_loss: 30537.7148\n",
      "Epoch 100/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3146.5303 - val_loss: 30613.1074\n",
      "Epoch 101/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3167.6189 - val_loss: 30470.2520\n",
      "Epoch 102/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3179.5176 - val_loss: 30124.2168\n",
      "Epoch 103/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3145.7200 - val_loss: 29696.4453\n",
      "Epoch 104/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3072.6125 - val_loss: 29299.3457\n",
      "Epoch 105/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2975.6702 - val_loss: 28980.0977\n",
      "Epoch 106/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2858.8740 - val_loss: 28733.1504\n",
      "Epoch 107/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2719.5989 - val_loss: 28546.2402\n",
      "Epoch 108/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2564.5029 - val_loss: 28414.5977\n",
      "Epoch 109/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2408.0254 - val_loss: 28314.7891\n",
      "Epoch 110/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2261.6697 - val_loss: 28207.9102\n",
      "Epoch 111/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2132.1526 - val_loss: 28078.6211\n",
      "Epoch 112/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2013.4954 - val_loss: 27855.0449\n",
      "Epoch 113/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1904.6256 - val_loss: 27562.1016\n",
      "Epoch 114/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1800.3669 - val_loss: 27208.5684\n",
      "Epoch 115/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1707.7103 - val_loss: 26779.9824\n",
      "Epoch 116/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1611.0171 - val_loss: 26347.3047\n",
      "Epoch 117/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1519.7501 - val_loss: 26002.5938\n",
      "Epoch 118/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1430.5541 - val_loss: 25792.8320\n",
      "Epoch 119/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1340.0593 - val_loss: 25720.0137\n",
      "Epoch 120/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1250.1427 - val_loss: 25758.0859\n",
      "Epoch 121/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1166.2891 - val_loss: 25863.1836\n",
      "Epoch 122/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1093.0740 - val_loss: 25980.4375\n",
      "Epoch 123/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1033.6420 - val_loss: 26069.0508\n",
      "Epoch 124/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 983.8062 - val_loss: 26117.3262\n",
      "Epoch 125/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 941.9429 - val_loss: 26146.4805\n",
      "Epoch 126/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 907.6943 - val_loss: 26188.6914\n",
      "Epoch 127/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 878.3315 - val_loss: 26287.4395\n",
      "Epoch 128/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 852.8156 - val_loss: 26471.9199\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 832.2686 - val_loss: 26742.3320\n",
      "Epoch 130/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 819.6772 - val_loss: 27067.5391\n",
      "Epoch 131/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 819.3824 - val_loss: 27388.5938\n",
      "Epoch 132/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 834.4182 - val_loss: 27630.9336\n",
      "Epoch 133/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 862.7003 - val_loss: 27731.0898\n",
      "Epoch 134/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 895.1283 - val_loss: 27673.5977\n",
      "Epoch 135/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 918.9992 - val_loss: 27514.4297\n",
      "Epoch 136/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 925.4199 - val_loss: 27380.7109\n",
      "Epoch 137/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 914.9109 - val_loss: 27455.6035\n",
      "Epoch 138/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 895.8817 - val_loss: 27946.6289\n",
      "Epoch 139/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 880.6730 - val_loss: 29019.3965\n",
      "Epoch 140/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 890.0825 - val_loss: 30661.2383\n",
      "Epoch 141/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 963.0969 - val_loss: 32498.9727\n",
      "Epoch 142/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1142.6653 - val_loss: 33696.9219\n",
      "Epoch 143/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1408.7676 - val_loss: 33199.5469\n",
      "Epoch 144/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1618.5704 - val_loss: 30615.8047\n",
      "Epoch 145/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1671.2020 - val_loss: 27127.7539\n",
      "Epoch 146/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1847.6951 - val_loss: 25093.5215\n",
      "Epoch 147/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2407.1074 - val_loss: 26667.5000\n",
      "Epoch 148/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2499.6882 - val_loss: 37365.6484\n",
      "Epoch 149/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2662.7368 - val_loss: 50122.3008\n",
      "Epoch 150/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3733.6511 - val_loss: 36636.6250\n",
      "Epoch 151/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3718.6011 - val_loss: 29603.6055\n",
      "Epoch 152/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2661.3223 - val_loss: 31012.7383\n",
      "Epoch 153/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1871.5410 - val_loss: 34654.7109\n",
      "Epoch 154/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1635.6426 - val_loss: 32757.4258\n",
      "Epoch 155/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1882.7490 - val_loss: 28808.4453\n",
      "Epoch 156/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1855.1962 - val_loss: 28731.6445\n",
      "Epoch 157/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1691.5659 - val_loss: 31629.1602\n",
      "Epoch 158/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1518.0220 - val_loss: 33170.3281\n",
      "Epoch 159/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1660.5931 - val_loss: 32028.9570\n",
      "Epoch 160/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1852.6434 - val_loss: 30820.0859\n",
      "Epoch 161/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1809.0391 - val_loss: 30931.6113\n",
      "Epoch 162/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1665.6560 - val_loss: 31196.0195\n",
      "Epoch 163/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1557.8080 - val_loss: 30700.6895\n",
      "Epoch 164/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1466.2014 - val_loss: 29781.0078\n",
      "Epoch 165/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1360.4275 - val_loss: 29547.9961\n",
      "Epoch 166/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1337.9753 - val_loss: 29881.0273\n",
      "Epoch 167/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1451.4609 - val_loss: 30323.7012\n",
      "Epoch 168/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1588.8478 - val_loss: 30532.3320\n",
      "Epoch 169/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1707.1582 - val_loss: 30416.8379\n",
      "Epoch 170/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1878.8170 - val_loss: 30321.4785\n",
      "Epoch 171/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2048.4478 - val_loss: 30135.6113\n",
      "Epoch 172/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2199.5779 - val_loss: 29431.9609\n",
      "Epoch 173/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2267.9331 - val_loss: 28596.9414\n",
      "Epoch 174/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2267.9392 - val_loss: 28891.4648\n",
      "Epoch 175/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2236.8420 - val_loss: 29875.0742\n",
      "Epoch 176/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2340.8862 - val_loss: 31094.4219\n",
      "Epoch 177/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2675.9736 - val_loss: 31896.9336\n",
      "Epoch 178/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3253.5171 - val_loss: 31319.8008\n",
      "Epoch 179/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3767.7544 - val_loss: 30925.7344\n",
      "Epoch 180/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4940.6621 - val_loss: 35038.2773\n",
      "Epoch 181/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6745.4316 - val_loss: 42663.0547\n",
      "Epoch 182/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9380.5312 - val_loss: 45767.0391\n",
      "Epoch 183/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11931.2900 - val_loss: 44949.5234\n",
      "Epoch 184/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12435.6436 - val_loss: 39529.8125\n",
      "Epoch 185/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11628.7959 - val_loss: 28449.5859\n",
      "Epoch 186/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8285.7188 - val_loss: 24599.7285\n",
      "Epoch 187/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5310.5703 - val_loss: 23725.3438\n",
      "Epoch 188/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3745.6133 - val_loss: 25725.0117\n",
      "Epoch 189/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2934.7742 - val_loss: 26157.0938\n",
      "Epoch 190/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2172.6692 - val_loss: 26016.6406\n",
      "Epoch 191/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1560.1851 - val_loss: 25720.4258\n",
      "Epoch 192/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1118.1375 - val_loss: 25373.1406\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 824.1467 - val_loss: 25063.3750\n",
      "Epoch 194/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 639.3488 - val_loss: 24856.8164\n",
      "Epoch 195/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 524.6058 - val_loss: 24701.6289\n",
      "Epoch 196/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 452.3016 - val_loss: 24587.9609\n",
      "Epoch 197/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 406.2595 - val_loss: 24493.7598\n",
      "Epoch 198/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 376.1150 - val_loss: 24420.6523\n",
      "Epoch 199/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 355.3536 - val_loss: 24373.8496\n",
      "Epoch 200/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 338.9999 - val_loss: 24349.6348\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11015.3330 \n",
      "Test Loss: 10734.330078125\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model6 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='swish', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model6.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train model\n",
    "history = model6.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss = model6.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "#predict\n",
    "predictions = model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd795cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prediction      Actual\n",
      "0    128.907257  119.793729\n",
      "1    281.091003  359.563884\n",
      "2     88.415672  168.934525\n",
      "3    381.747375  355.505368\n",
      "4    402.639435  433.151080\n",
      "..          ...         ...\n",
      "112  585.799194  487.957347\n",
      "113  198.923340  216.123044\n",
      "114  248.870346  425.953734\n",
      "115  377.582764  436.229287\n",
      "116  494.045013  388.424770\n",
      "\n",
      "[117 rows x 2 columns]\n",
      "Mean Absolute Error (MAE): 75.61835687994024\n",
      "Mean Squared Error (MSE): 10442.31928837417\n",
      "Root Mean Squared Error (RMSE): 102.18766700719893\n",
      "R-squared (R²): 0.8151427865295936\n",
      "Mean Absolute Error: 75.62 degrees.\n",
      "Accuracy: 73.28 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if predictions.ndim != 1:\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "# Check if 'y_test' is not 1-dimensional and reshape if necessary\n",
    "if y_test.ndim != 1:\n",
    "    y_test = y_test.flatten()\n",
    "    \n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "print(results)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(mae, 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48548329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "43       43    0.201448\n",
      "41       41    0.197360\n",
      "1         1    0.196766\n",
      "28       28    0.191743\n",
      "5         5    0.190190\n",
      "26       26    0.189344\n",
      "30       30    0.185879\n",
      "40       40    0.183141\n",
      "24       24    0.179078\n",
      "13       13    0.178402\n",
      "32       32    0.175963\n",
      "44       44    0.173091\n",
      "48       48    0.172633\n",
      "7         7    0.171995\n",
      "38       38    0.169064\n",
      "49       49    0.169024\n",
      "22       22    0.167335\n",
      "34       34    0.158822\n",
      "36       36    0.158515\n",
      "9         9    0.157048\n",
      "3         3    0.155340\n",
      "20       20    0.147847\n",
      "53       53    0.145566\n",
      "11       11    0.141075\n",
      "16       16    0.139078\n",
      "54       54    0.134611\n",
      "0         0    0.133079\n",
      "52       52    0.130699\n",
      "18       18    0.125669\n",
      "51       51    0.123983\n",
      "8         8    0.123366\n",
      "12       12    0.123104\n",
      "50       50    0.118709\n",
      "35       35    0.117839\n",
      "2         2    0.116204\n",
      "46       46    0.116088\n",
      "55       55    0.115000\n",
      "23       23    0.113072\n",
      "14       14    0.108989\n",
      "42       42    0.108963\n",
      "27       27    0.107148\n",
      "4         4    0.106769\n",
      "39       39    0.106672\n",
      "37       37    0.106646\n",
      "56       56    0.104743\n",
      "10       10    0.104290\n",
      "47       47    0.103328\n",
      "6         6    0.102860\n",
      "33       33    0.102332\n",
      "19       19    0.102135\n",
      "45       45    0.102086\n",
      "31       31    0.101614\n",
      "21       21    0.100840\n",
      "15       15    0.100417\n",
      "17       17    0.098381\n",
      "29       29    0.097834\n",
      "25       25    0.096907\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute weights of the first layer\n",
    "weights = model6.layers[0].get_weights()[0]\n",
    "abs_weights = np.abs(weights).mean(axis=1)\n",
    "\n",
    "# Summarize feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': range(X_train.shape[1]), 'Importance': abs_weights})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346ef89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model6.save('NN-81.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46a4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 232395.5781 - val_loss: 211149.9375\n",
      "Epoch 2/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 167649.5156 - val_loss: 90040.7500\n",
      "Epoch 3/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100965.1953 - val_loss: 62892.0781\n",
      "Epoch 4/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80400.8828 - val_loss: 50917.7266\n",
      "Epoch 5/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66857.0469 - val_loss: 43461.2617\n",
      "Epoch 6/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57232.3281 - val_loss: 38881.9336\n",
      "Epoch 7/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49740.8242 - val_loss: 36073.1328\n",
      "Epoch 8/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43543.2891 - val_loss: 33825.4844\n",
      "Epoch 9/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38950.7070 - val_loss: 32151.3848\n",
      "Epoch 10/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34985.7383 - val_loss: 30820.0273\n",
      "Epoch 11/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32337.1758 - val_loss: 29347.7129\n",
      "Epoch 12/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29882.5762 - val_loss: 28388.9902\n",
      "Epoch 13/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27435.1055 - val_loss: 27720.5098\n",
      "Epoch 14/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25063.7207 - val_loss: 26251.9355\n",
      "Epoch 15/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23018.7773 - val_loss: 25940.9922\n",
      "Epoch 16/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21257.6016 - val_loss: 25543.2148\n",
      "Epoch 17/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20160.3320 - val_loss: 25258.9180\n",
      "Epoch 18/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19099.9297 - val_loss: 25143.9512\n",
      "Epoch 19/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18323.1328 - val_loss: 25076.4863\n",
      "Epoch 20/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17656.4082 - val_loss: 25053.4590\n",
      "Epoch 21/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17075.2402 - val_loss: 25042.5977\n",
      "Epoch 22/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16552.8613 - val_loss: 25036.4551\n",
      "Epoch 23/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16070.3994 - val_loss: 25087.0254\n",
      "Epoch 24/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15625.3984 - val_loss: 25121.0156\n",
      "Epoch 25/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15205.8398 - val_loss: 25167.4141\n",
      "Epoch 26/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14802.5820 - val_loss: 25219.6035\n",
      "Epoch 27/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14407.9141 - val_loss: 25296.5996\n",
      "Epoch 28/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14019.6143 - val_loss: 25391.5781\n",
      "Epoch 29/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13639.0918 - val_loss: 25491.3887\n",
      "Epoch 30/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13269.5488 - val_loss: 25582.8652\n",
      "Epoch 31/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12902.6182 - val_loss: 25688.9258\n",
      "Epoch 32/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12538.4111 - val_loss: 25811.8438\n",
      "Epoch 33/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12180.5771 - val_loss: 25938.9023\n",
      "Epoch 34/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11825.0361 - val_loss: 26112.5859\n",
      "Epoch 35/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11483.5215 - val_loss: 26235.3789\n",
      "Epoch 36/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11129.6191 - val_loss: 26421.8418\n",
      "Epoch 37/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10791.4111 - val_loss: 26536.3887\n",
      "Epoch 38/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10433.5664 - val_loss: 26753.0117\n",
      "Epoch 39/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10121.3613 - val_loss: 26897.7734\n",
      "Epoch 40/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9794.1436 - val_loss: 27019.3574\n",
      "Epoch 41/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9466.4180 - val_loss: 27137.8145\n",
      "Epoch 42/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9150.7070 - val_loss: 27248.2578\n",
      "Epoch 43/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8846.8164 - val_loss: 27345.5820\n",
      "Epoch 44/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8555.6348 - val_loss: 27439.5273\n",
      "Epoch 45/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8280.7568 - val_loss: 27521.1211\n",
      "Epoch 46/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8021.7544 - val_loss: 27595.6855\n",
      "Epoch 47/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7781.2554 - val_loss: 27657.7461\n",
      "Epoch 48/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7558.0078 - val_loss: 27716.0059\n",
      "Epoch 49/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7350.5801 - val_loss: 27752.7129\n",
      "Epoch 50/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7156.6470 - val_loss: 27765.7383\n",
      "Epoch 51/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6971.0566 - val_loss: 27750.5977\n",
      "Epoch 52/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6793.3965 - val_loss: 27707.8223\n",
      "Epoch 53/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6617.1973 - val_loss: 27629.8086\n",
      "Epoch 54/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6437.8809 - val_loss: 27522.6367\n",
      "Epoch 55/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6251.4946 - val_loss: 27384.8809\n",
      "Epoch 56/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6054.4194 - val_loss: 27232.6270\n",
      "Epoch 57/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5845.1206 - val_loss: 27072.3594\n",
      "Epoch 58/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5623.2065 - val_loss: 26919.5723\n",
      "Epoch 59/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5391.6938 - val_loss: 26780.4141\n",
      "Epoch 60/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5156.1123 - val_loss: 26662.1699\n",
      "Epoch 61/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4922.9277 - val_loss: 26563.3184\n",
      "Epoch 62/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4698.4277 - val_loss: 26483.0859\n",
      "Epoch 63/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4487.8203 - val_loss: 26417.6680\n",
      "Epoch 64/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4295.0967 - val_loss: 26369.5195\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4122.5156 - val_loss: 26340.2852\n",
      "Epoch 66/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3971.2231 - val_loss: 26313.9805\n",
      "Epoch 67/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3838.3435 - val_loss: 26248.5547\n",
      "Epoch 68/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3711.4739 - val_loss: 26131.3457\n",
      "Epoch 69/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3584.3997 - val_loss: 25999.0684\n",
      "Epoch 70/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3475.9565 - val_loss: 25884.3867\n",
      "Epoch 71/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3389.5662 - val_loss: 25792.8789\n",
      "Epoch 72/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3309.0642 - val_loss: 25716.1445\n",
      "Epoch 73/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3220.6841 - val_loss: 25654.9668\n",
      "Epoch 74/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3119.9426 - val_loss: 25613.9336\n",
      "Epoch 75/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3013.3167 - val_loss: 25607.4238\n",
      "Epoch 76/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2905.5786 - val_loss: 25648.3398\n",
      "Epoch 77/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2799.0381 - val_loss: 25739.4805\n",
      "Epoch 78/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2695.8481 - val_loss: 25881.7500\n",
      "Epoch 79/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2598.7356 - val_loss: 26079.0000\n",
      "Epoch 80/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2509.8452 - val_loss: 26333.7012\n",
      "Epoch 81/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2430.4648 - val_loss: 26649.0742\n",
      "Epoch 82/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2362.8059 - val_loss: 27029.7383\n",
      "Epoch 83/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2310.8640 - val_loss: 27476.7266\n",
      "Epoch 84/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2279.8772 - val_loss: 27974.2031\n",
      "Epoch 85/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2273.3755 - val_loss: 28489.0996\n",
      "Epoch 86/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2292.2842 - val_loss: 29006.7461\n",
      "Epoch 87/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2354.3047 - val_loss: 29538.6387\n",
      "Epoch 88/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2518.3679 - val_loss: 30086.9785\n",
      "Epoch 89/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2825.5916 - val_loss: 30563.5059\n",
      "Epoch 90/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3256.2412 - val_loss: 30753.8477\n",
      "Epoch 91/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3861.0859 - val_loss: 29922.3047\n",
      "Epoch 92/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4725.6250 - val_loss: 27381.3828\n",
      "Epoch 93/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5636.9751 - val_loss: 25765.3438\n",
      "Epoch 94/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6380.5981 - val_loss: 26685.8535\n",
      "Epoch 95/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6591.9800 - val_loss: 27925.4746\n",
      "Epoch 96/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5176.1265 - val_loss: 32547.0449\n",
      "Epoch 97/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5388.8052 - val_loss: 34195.3711\n",
      "Epoch 98/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6066.9487 - val_loss: 35162.0977\n",
      "Epoch 99/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6151.3022 - val_loss: 34609.0781\n",
      "Epoch 100/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5732.2959 - val_loss: 32913.9062\n",
      "Epoch 101/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4855.0195 - val_loss: 31586.8652\n",
      "Epoch 102/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3939.7053 - val_loss: 30613.5488\n",
      "Epoch 103/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3300.5686 - val_loss: 29950.5039\n",
      "Epoch 104/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2895.7703 - val_loss: 29565.7539\n",
      "Epoch 105/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2579.6526 - val_loss: 29345.8320\n",
      "Epoch 106/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2312.0188 - val_loss: 29208.5254\n",
      "Epoch 107/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2087.2754 - val_loss: 29115.9961\n",
      "Epoch 108/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1900.7983 - val_loss: 29052.8652\n",
      "Epoch 109/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1746.7612 - val_loss: 29017.3828\n",
      "Epoch 110/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1618.3423 - val_loss: 29022.5469\n",
      "Epoch 111/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1511.0531 - val_loss: 29072.1914\n",
      "Epoch 112/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1420.9933 - val_loss: 29166.4492\n",
      "Epoch 113/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1344.6296 - val_loss: 29304.2656\n",
      "Epoch 114/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1279.1738 - val_loss: 29486.4062\n",
      "Epoch 115/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1222.6937 - val_loss: 29714.2988\n",
      "Epoch 116/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1173.9402 - val_loss: 29988.0273\n",
      "Epoch 117/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1131.9669 - val_loss: 30306.1836\n",
      "Epoch 118/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1095.9877 - val_loss: 30666.9121\n",
      "Epoch 119/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1065.3011 - val_loss: 31068.5020\n",
      "Epoch 120/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1039.2573 - val_loss: 31509.5449\n",
      "Epoch 121/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1017.2695 - val_loss: 31988.7344\n",
      "Epoch 122/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 998.8324 - val_loss: 32504.7285\n",
      "Epoch 123/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 983.5782 - val_loss: 33056.4609\n",
      "Epoch 124/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 971.2917 - val_loss: 33643.3281\n",
      "Epoch 125/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 961.8817 - val_loss: 34265.6328\n",
      "Epoch 126/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 955.3702 - val_loss: 34924.6328\n",
      "Epoch 127/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 951.8813 - val_loss: 35622.7344\n",
      "Epoch 128/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 951.6790 - val_loss: 36363.4531\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 955.1821 - val_loss: 37151.5156\n",
      "Epoch 130/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 962.9976 - val_loss: 37992.4062\n",
      "Epoch 131/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 975.8938 - val_loss: 38892.2422\n",
      "Epoch 132/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 994.8049 - val_loss: 39857.2227\n",
      "Epoch 133/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1020.7841 - val_loss: 40893.0273\n",
      "Epoch 134/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1055.0520 - val_loss: 42004.4102\n",
      "Epoch 135/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1099.1907 - val_loss: 43196.2812\n",
      "Epoch 136/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1155.7354 - val_loss: 44478.3203\n",
      "Epoch 137/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1229.3392 - val_loss: 45868.0234\n",
      "Epoch 138/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1326.2074 - val_loss: 47364.8164\n",
      "Epoch 139/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1446.4783 - val_loss: 48967.7031\n",
      "Epoch 140/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1598.0023 - val_loss: 50838.6484\n",
      "Epoch 141/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1769.8112 - val_loss: 52944.4258\n",
      "Epoch 142/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2023.2863 - val_loss: 55023.3086\n",
      "Epoch 143/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2196.4207 - val_loss: 57312.7227\n",
      "Epoch 144/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2543.5942 - val_loss: 59740.3320\n",
      "Epoch 145/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2916.5891 - val_loss: 60086.2812\n",
      "Epoch 146/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3692.0793 - val_loss: 55114.6250\n",
      "Epoch 147/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4424.1260 - val_loss: 45995.3555\n",
      "Epoch 148/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5257.0073 - val_loss: 38677.4180\n",
      "Epoch 149/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4965.3086 - val_loss: 44570.8164\n",
      "Epoch 150/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6984.2861 - val_loss: 37397.6055\n",
      "Epoch 151/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6386.6807 - val_loss: 31729.8301\n",
      "Epoch 152/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4346.7778 - val_loss: 33493.1055\n",
      "Epoch 153/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3052.6616 - val_loss: 34804.4062\n",
      "Epoch 154/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2421.2764 - val_loss: 35706.2422\n",
      "Epoch 155/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2218.5586 - val_loss: 35964.7422\n",
      "Epoch 156/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2171.4963 - val_loss: 35598.5625\n",
      "Epoch 157/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2235.1550 - val_loss: 34811.3594\n",
      "Epoch 158/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2420.0125 - val_loss: 33920.7656\n",
      "Epoch 159/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2727.3318 - val_loss: 33004.4609\n",
      "Epoch 160/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3202.0981 - val_loss: 32480.6680\n",
      "Epoch 161/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3946.4866 - val_loss: 32669.8750\n",
      "Epoch 162/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5022.1621 - val_loss: 33980.0352\n",
      "Epoch 163/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6353.1323 - val_loss: 36446.7852\n",
      "Epoch 164/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7719.1089 - val_loss: 38612.2383\n",
      "Epoch 165/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8936.0547 - val_loss: 38091.4922\n",
      "Epoch 166/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9237.4209 - val_loss: 34390.8594\n",
      "Epoch 167/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8279.2578 - val_loss: 29132.2891\n",
      "Epoch 168/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6251.8125 - val_loss: 25908.4883\n",
      "Epoch 169/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4226.0996 - val_loss: 25173.7227\n",
      "Epoch 170/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2926.4624 - val_loss: 25414.3711\n",
      "Epoch 171/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2152.1814 - val_loss: 25756.4648\n",
      "Epoch 172/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1604.1915 - val_loss: 26076.5957\n",
      "Epoch 173/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1189.1793 - val_loss: 26300.6895\n",
      "Epoch 174/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 897.1223 - val_loss: 26403.6406\n",
      "Epoch 175/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 701.8350 - val_loss: 26448.1309\n",
      "Epoch 176/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 574.7025 - val_loss: 26463.1465\n",
      "Epoch 177/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 491.7314 - val_loss: 26467.3867\n",
      "Epoch 178/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.6044 - val_loss: 26462.0332\n",
      "Epoch 179/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 405.7975 - val_loss: 26451.3477\n",
      "Epoch 180/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 386.6577 - val_loss: 26435.4102\n",
      "Epoch 181/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 376.1822 - val_loss: 26412.8379\n",
      "Epoch 182/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 370.5354 - val_loss: 26383.7617\n",
      "Epoch 183/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 367.1016 - val_loss: 26349.0273\n",
      "Epoch 184/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 364.1702 - val_loss: 26309.1836\n",
      "Epoch 185/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 360.5089 - val_loss: 26265.3105\n",
      "Epoch 186/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 355.2540 - val_loss: 26219.5664\n",
      "Epoch 187/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 347.9512 - val_loss: 26174.3379\n",
      "Epoch 188/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 338.5763 - val_loss: 26131.4590\n",
      "Epoch 189/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 327.3857 - val_loss: 26092.7539\n",
      "Epoch 190/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 314.7275 - val_loss: 26060.4648\n",
      "Epoch 191/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 300.9586 - val_loss: 26037.0117\n",
      "Epoch 192/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 286.4410 - val_loss: 26024.3633\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 271.5472 - val_loss: 26023.8281\n",
      "Epoch 194/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 256.6434 - val_loss: 26036.2480\n",
      "Epoch 195/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242.0623 - val_loss: 26062.2383\n",
      "Epoch 196/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 228.0752 - val_loss: 26102.2734\n",
      "Epoch 197/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 214.8785 - val_loss: 26156.7891\n",
      "Epoch 198/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 202.5938 - val_loss: 26226.1680\n",
      "Epoch 199/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 191.2838 - val_loss: 26310.6211\n",
      "Epoch 200/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 180.9633 - val_loss: 26410.0840\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11622.2471 \n",
      "Test Loss: 10780.95703125\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model7 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='swish', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model7.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train model\n",
    "history = model7.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss = model7.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "#predict\n",
    "predictions = model7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feedce4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prediction      Actual\n",
      "0    154.194443  119.793729\n",
      "1    314.775574  359.563884\n",
      "2    106.809914  168.934525\n",
      "3    424.520447  355.505368\n",
      "4    437.852661  433.151080\n",
      "..          ...         ...\n",
      "112  588.582886  487.957347\n",
      "113  231.066681  216.123044\n",
      "114  408.025146  425.953734\n",
      "115  365.146423  436.229287\n",
      "116  449.862854  388.424770\n",
      "\n",
      "[117 rows x 2 columns]\n",
      "Mean Absolute Error (MAE): 74.9939545292641\n",
      "Mean Squared Error (MSE): 10684.65904716702\n",
      "Root Mean Squared Error (RMSE): 103.36662443539026\n",
      "R-squared (R²): 0.8108527192287965\n",
      "Mean Absolute Error: 74.99 degrees.\n",
      "Accuracy: 73.24 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if predictions.ndim != 1:\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "# Check if 'y_test' is not 1-dimensional and reshape if necessary\n",
    "if y_test.ndim != 1:\n",
    "    y_test = y_test.flatten()\n",
    "    \n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "print(results)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(mae, 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685d1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model7.save('83my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ac2a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "41       41    0.206163\n",
      "5         5    0.198837\n",
      "43       43    0.194183\n",
      "44       44    0.190467\n",
      "30       30    0.189696\n",
      "38       38    0.182575\n",
      "28       28    0.180320\n",
      "9         9    0.177956\n",
      "1         1    0.177795\n",
      "32       32    0.177467\n",
      "40       40    0.175643\n",
      "26       26    0.173614\n",
      "49       49    0.172976\n",
      "22       22    0.168847\n",
      "48       48    0.165313\n",
      "34       34    0.162359\n",
      "13       13    0.157547\n",
      "20       20    0.157539\n",
      "7         7    0.156311\n",
      "24       24    0.155565\n",
      "36       36    0.154309\n",
      "11       11    0.152198\n",
      "16       16    0.149351\n",
      "18       18    0.144675\n",
      "3         3    0.140338\n",
      "54       54    0.136956\n",
      "53       53    0.136029\n",
      "52       52    0.135709\n",
      "51       51    0.129983\n",
      "50       50    0.125207\n",
      "0         0    0.118034\n",
      "19       19    0.116703\n",
      "17       17    0.116671\n",
      "45       45    0.112965\n",
      "12       12    0.112322\n",
      "55       55    0.111157\n",
      "4         4    0.110562\n",
      "37       37    0.109569\n",
      "14       14    0.109277\n",
      "46       46    0.108142\n",
      "39       39    0.107163\n",
      "33       33    0.107109\n",
      "42       42    0.106845\n",
      "2         2    0.106739\n",
      "15       15    0.105996\n",
      "47       47    0.104913\n",
      "35       35    0.104728\n",
      "8         8    0.104314\n",
      "27       27    0.104157\n",
      "23       23    0.103904\n",
      "31       31    0.103601\n",
      "6         6    0.102725\n",
      "21       21    0.102526\n",
      "10       10    0.101451\n",
      "25       25    0.100589\n",
      "56       56    0.099949\n",
      "29       29    0.093754\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute weights of the first layer\n",
    "weights = model7.layers[0].get_weights()[0]\n",
    "abs_weights = np.abs(weights).mean(axis=1)\n",
    "\n",
    "# Summarize feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': range(X_train.shape[1]), 'Importance': abs_weights})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b11287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 232945.3906 - val_loss: 217680.0000\n",
      "Epoch 2/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 172680.4375 - val_loss: 91452.1250\n",
      "Epoch 3/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98966.2422 - val_loss: 60749.2656\n",
      "Epoch 4/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78104.0000 - val_loss: 50026.9688\n",
      "Epoch 5/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65900.6094 - val_loss: 43225.6992\n",
      "Epoch 6/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56002.4102 - val_loss: 38769.2812\n",
      "Epoch 7/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49370.5977 - val_loss: 35752.1797\n",
      "Epoch 8/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42615.5391 - val_loss: 33161.3672\n",
      "Epoch 9/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38599.4453 - val_loss: 31300.0586\n",
      "Epoch 10/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35004.5586 - val_loss: 29839.1309\n",
      "Epoch 11/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32304.6445 - val_loss: 28276.2988\n",
      "Epoch 12/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29627.8906 - val_loss: 26867.8574\n",
      "Epoch 13/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26924.4785 - val_loss: 25159.1953\n",
      "Epoch 14/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24559.6387 - val_loss: 24104.3477\n",
      "Epoch 15/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22729.7930 - val_loss: 23747.2285\n",
      "Epoch 16/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21336.1230 - val_loss: 23589.4941\n",
      "Epoch 17/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20210.1855 - val_loss: 23332.1016\n",
      "Epoch 18/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19305.6797 - val_loss: 23231.7227\n",
      "Epoch 19/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18596.4570 - val_loss: 23013.2715\n",
      "Epoch 20/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17928.1719 - val_loss: 22943.1113\n",
      "Epoch 21/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17385.8359 - val_loss: 22799.9102\n",
      "Epoch 22/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16875.7324 - val_loss: 22742.0488\n",
      "Epoch 23/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16405.1309 - val_loss: 22639.8633\n",
      "Epoch 24/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15949.9209 - val_loss: 22583.2305\n",
      "Epoch 25/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15521.8809 - val_loss: 22505.6504\n",
      "Epoch 26/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15107.6641 - val_loss: 22468.6191\n",
      "Epoch 27/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14707.6553 - val_loss: 22424.4062\n",
      "Epoch 28/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14315.7998 - val_loss: 22410.6074\n",
      "Epoch 29/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13929.6602 - val_loss: 22393.0625\n",
      "Epoch 30/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13547.7188 - val_loss: 22405.7812\n",
      "Epoch 31/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13176.3799 - val_loss: 22436.4141\n",
      "Epoch 32/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12809.7500 - val_loss: 22498.9707\n",
      "Epoch 33/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12452.1260 - val_loss: 22568.0840\n",
      "Epoch 34/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12100.5371 - val_loss: 22640.8750\n",
      "Epoch 35/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11754.2314 - val_loss: 22702.9805\n",
      "Epoch 36/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11411.1113 - val_loss: 22753.7383\n",
      "Epoch 37/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11070.4258 - val_loss: 22785.5840\n",
      "Epoch 38/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10732.4961 - val_loss: 22795.1797\n",
      "Epoch 39/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10396.7998 - val_loss: 22780.5625\n",
      "Epoch 40/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10064.4785 - val_loss: 22739.4414\n",
      "Epoch 41/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9735.8154 - val_loss: 22674.5195\n",
      "Epoch 42/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9411.8799 - val_loss: 22585.4258\n",
      "Epoch 43/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9092.2637 - val_loss: 22476.9316\n",
      "Epoch 44/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8777.2480 - val_loss: 22359.9648\n",
      "Epoch 45/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8471.7178 - val_loss: 22240.2402\n",
      "Epoch 46/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8177.2070 - val_loss: 22115.0898\n",
      "Epoch 47/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7892.2437 - val_loss: 22005.2383\n",
      "Epoch 48/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7621.1416 - val_loss: 21893.5977\n",
      "Epoch 49/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7359.4048 - val_loss: 21790.7070\n",
      "Epoch 50/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7107.8208 - val_loss: 21722.5859\n",
      "Epoch 51/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6871.0903 - val_loss: 21663.0703\n",
      "Epoch 52/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6644.4023 - val_loss: 21618.8359\n",
      "Epoch 53/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6428.8740 - val_loss: 21584.7852\n",
      "Epoch 54/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6220.6460 - val_loss: 21561.6172\n",
      "Epoch 55/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6019.9214 - val_loss: 21550.1934\n",
      "Epoch 56/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5825.3267 - val_loss: 21550.4648\n",
      "Epoch 57/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5635.6650 - val_loss: 21562.8066\n",
      "Epoch 58/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5450.8252 - val_loss: 21587.3691\n",
      "Epoch 59/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5270.5771 - val_loss: 21624.9375\n",
      "Epoch 60/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5094.4702 - val_loss: 21677.4473\n",
      "Epoch 61/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4922.1265 - val_loss: 21746.5117\n",
      "Epoch 62/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4753.2969 - val_loss: 21832.4961\n",
      "Epoch 63/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4586.1572 - val_loss: 21936.4707\n",
      "Epoch 64/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4422.0986 - val_loss: 22045.1855\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4263.5352 - val_loss: 22176.0000\n",
      "Epoch 66/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4109.5830 - val_loss: 22311.6934\n",
      "Epoch 67/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3960.0273 - val_loss: 22485.0586\n",
      "Epoch 68/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3819.3982 - val_loss: 22655.8594\n",
      "Epoch 69/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3674.8335 - val_loss: 22306.1641\n",
      "Epoch 70/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3547.0889 - val_loss: 23238.6680\n",
      "Epoch 71/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3479.8845 - val_loss: 22919.8008\n",
      "Epoch 72/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3402.7324 - val_loss: 22979.4746\n",
      "Epoch 73/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3306.2053 - val_loss: 23266.9160\n",
      "Epoch 74/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3174.8013 - val_loss: 23466.9199\n",
      "Epoch 75/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3121.6677 - val_loss: 23542.0547\n",
      "Epoch 76/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3081.7830 - val_loss: 23674.7734\n",
      "Epoch 77/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3059.8418 - val_loss: 23807.9062\n",
      "Epoch 78/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3076.8354 - val_loss: 23834.6660\n",
      "Epoch 79/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3147.4849 - val_loss: 23600.4570\n",
      "Epoch 80/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3292.6907 - val_loss: 22991.9453\n",
      "Epoch 81/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3546.7126 - val_loss: 22003.5762\n",
      "Epoch 82/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3971.0015 - val_loss: 20992.5332\n",
      "Epoch 83/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4583.1255 - val_loss: 20633.9375\n",
      "Epoch 84/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5276.3560 - val_loss: 21327.0566\n",
      "Epoch 85/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6606.9824 - val_loss: 21363.6250\n",
      "Epoch 86/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7181.9937 - val_loss: 22936.9941\n",
      "Epoch 87/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4803.1445 - val_loss: 22690.5977\n",
      "Epoch 88/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3684.6609 - val_loss: 21455.9219\n",
      "Epoch 89/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3358.6799 - val_loss: 21829.7422\n",
      "Epoch 90/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3256.5413 - val_loss: 22830.9121\n",
      "Epoch 91/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3365.1016 - val_loss: 24156.7070\n",
      "Epoch 92/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3535.4810 - val_loss: 25099.4395\n",
      "Epoch 93/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3586.5427 - val_loss: 25749.3438\n",
      "Epoch 94/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3596.6279 - val_loss: 26237.2754\n",
      "Epoch 95/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3605.4290 - val_loss: 26713.4160\n",
      "Epoch 96/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3623.9824 - val_loss: 27162.2793\n",
      "Epoch 97/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3643.2900 - val_loss: 27543.5781\n",
      "Epoch 98/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3654.0139 - val_loss: 27716.7148\n",
      "Epoch 99/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3650.1851 - val_loss: 27650.7031\n",
      "Epoch 100/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3640.2180 - val_loss: 27264.0742\n",
      "Epoch 101/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3563.7283 - val_loss: 26537.1641\n",
      "Epoch 102/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3430.2031 - val_loss: 25650.4414\n",
      "Epoch 103/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3238.9314 - val_loss: 24668.3535\n",
      "Epoch 104/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3020.5945 - val_loss: 23678.9863\n",
      "Epoch 105/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2794.3250 - val_loss: 23003.1738\n",
      "Epoch 106/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2588.2876 - val_loss: 22447.2715\n",
      "Epoch 107/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2434.0667 - val_loss: 22166.2676\n",
      "Epoch 108/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2268.4333 - val_loss: 22048.4258\n",
      "Epoch 109/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2120.3325 - val_loss: 22124.4688\n",
      "Epoch 110/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1977.2448 - val_loss: 22248.8398\n",
      "Epoch 111/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1845.5031 - val_loss: 22363.5879\n",
      "Epoch 112/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1724.7454 - val_loss: 22437.8516\n",
      "Epoch 113/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1613.3218 - val_loss: 22500.0039\n",
      "Epoch 114/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1509.5438 - val_loss: 22567.4355\n",
      "Epoch 115/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1410.3556 - val_loss: 22652.8086\n",
      "Epoch 116/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1314.2390 - val_loss: 22758.6953\n",
      "Epoch 117/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1223.1648 - val_loss: 22885.6484\n",
      "Epoch 118/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1141.8658 - val_loss: 23029.7539\n",
      "Epoch 119/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1074.1173 - val_loss: 23189.3594\n",
      "Epoch 120/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1021.1155 - val_loss: 23365.9375\n",
      "Epoch 121/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 982.3868 - val_loss: 23565.3438\n",
      "Epoch 122/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 956.8522 - val_loss: 23798.3184\n",
      "Epoch 123/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 943.5895 - val_loss: 24076.3145\n",
      "Epoch 124/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 942.5616 - val_loss: 24402.3555\n",
      "Epoch 125/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 954.2075 - val_loss: 24764.7168\n",
      "Epoch 126/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 977.3700 - val_loss: 25139.6172\n",
      "Epoch 127/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1007.7464 - val_loss: 25496.0586\n",
      "Epoch 128/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1037.9167 - val_loss: 25798.5684\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1059.6541 - val_loss: 26020.5977\n",
      "Epoch 130/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1068.7955 - val_loss: 26153.8711\n",
      "Epoch 131/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1067.1234 - val_loss: 26215.3887\n",
      "Epoch 132/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1060.7018 - val_loss: 26247.0957\n",
      "Epoch 133/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1056.0442 - val_loss: 26308.9297\n",
      "Epoch 134/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1057.1769 - val_loss: 26464.0039\n",
      "Epoch 135/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1064.9985 - val_loss: 26759.6387\n",
      "Epoch 136/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1079.3724 - val_loss: 27218.9414\n",
      "Epoch 137/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1102.8110 - val_loss: 27855.7129\n",
      "Epoch 138/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1137.8512 - val_loss: 28557.1172\n",
      "Epoch 139/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1200.1411 - val_loss: 29213.7012\n",
      "Epoch 140/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1304.5154 - val_loss: 29487.3848\n",
      "Epoch 141/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1455.6439 - val_loss: 28909.7988\n",
      "Epoch 142/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1635.6011 - val_loss: 27247.9121\n",
      "Epoch 143/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1838.4226 - val_loss: 24731.7422\n",
      "Epoch 144/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2086.5405 - val_loss: 22781.9922\n",
      "Epoch 145/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2423.3174 - val_loss: 23607.7891\n",
      "Epoch 146/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2771.1094 - val_loss: 29594.0469\n",
      "Epoch 147/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3413.0581 - val_loss: 34354.2070\n",
      "Epoch 148/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5072.8892 - val_loss: 26278.8340\n",
      "Epoch 149/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4689.9521 - val_loss: 23952.1328\n",
      "Epoch 150/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4235.2070 - val_loss: 25070.6348\n",
      "Epoch 151/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3478.1836 - val_loss: 23964.9727\n",
      "Epoch 152/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3031.3169 - val_loss: 23455.7207\n",
      "Epoch 153/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2907.4304 - val_loss: 24548.8945\n",
      "Epoch 154/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3072.3540 - val_loss: 24130.2324\n",
      "Epoch 155/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3113.0486 - val_loss: 22241.4707\n",
      "Epoch 156/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2915.4036 - val_loss: 22257.9922\n",
      "Epoch 157/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2778.2659 - val_loss: 21567.8809\n",
      "Epoch 158/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2858.4192 - val_loss: 21293.0371\n",
      "Epoch 159/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3423.1985 - val_loss: 20543.7832\n",
      "Epoch 160/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3757.1218 - val_loss: 20961.1602\n",
      "Epoch 161/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4124.6558 - val_loss: 20174.1797\n",
      "Epoch 162/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4049.1826 - val_loss: 20078.9023\n",
      "Epoch 163/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4356.0576 - val_loss: 22008.5566\n",
      "Epoch 164/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5378.5044 - val_loss: 25595.0566\n",
      "Epoch 165/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7624.3247 - val_loss: 28058.5098\n",
      "Epoch 166/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9697.2129 - val_loss: 30045.1211\n",
      "Epoch 167/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10383.4424 - val_loss: 27793.2637\n",
      "Epoch 168/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10077.8447 - val_loss: 23326.7988\n",
      "Epoch 169/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8362.3965 - val_loss: 18770.1348\n",
      "Epoch 170/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5598.1831 - val_loss: 16731.2207\n",
      "Epoch 171/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3693.4756 - val_loss: 16469.6250\n",
      "Epoch 172/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2466.2363 - val_loss: 16757.2871\n",
      "Epoch 173/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1811.1140 - val_loss: 16955.6855\n",
      "Epoch 174/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1361.7208 - val_loss: 17027.7070\n",
      "Epoch 175/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1035.5890 - val_loss: 17006.8027\n",
      "Epoch 176/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 803.3691 - val_loss: 16981.0605\n",
      "Epoch 177/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 651.1706 - val_loss: 16938.6992\n",
      "Epoch 178/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 551.0948 - val_loss: 16892.9062\n",
      "Epoch 179/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 486.6231 - val_loss: 16847.4082\n",
      "Epoch 180/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 443.0355 - val_loss: 16784.7148\n",
      "Epoch 181/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 412.6779 - val_loss: 16732.0156\n",
      "Epoch 182/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 392.8878 - val_loss: 16696.7793\n",
      "Epoch 183/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 380.1280 - val_loss: 16681.1289\n",
      "Epoch 184/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 370.7416 - val_loss: 16682.7969\n",
      "Epoch 185/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 362.7785 - val_loss: 16694.5117\n",
      "Epoch 186/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 354.7458 - val_loss: 16715.0371\n",
      "Epoch 187/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 346.0949 - val_loss: 16741.5195\n",
      "Epoch 188/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 336.4459 - val_loss: 16772.2656\n",
      "Epoch 189/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 325.5270 - val_loss: 16805.2871\n",
      "Epoch 190/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 313.3822 - val_loss: 16838.2617\n",
      "Epoch 191/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 300.1441 - val_loss: 16870.8906\n",
      "Epoch 192/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 286.1218 - val_loss: 16901.8809\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271.6102 - val_loss: 16931.5840\n",
      "Epoch 194/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 256.9312 - val_loss: 16959.9082\n",
      "Epoch 195/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 242.3417 - val_loss: 16986.4199\n",
      "Epoch 196/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 228.0712 - val_loss: 17012.1836\n",
      "Epoch 197/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 214.3212 - val_loss: 17036.0977\n",
      "Epoch 198/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 201.1813 - val_loss: 17059.3242\n",
      "Epoch 199/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 188.8326 - val_loss: 17081.8164\n",
      "Epoch 200/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 177.2595 - val_loss: 17102.4258\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 12436.2686 \n",
      "Test Loss: 11381.5478515625\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020BFF4E0720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 64ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020BFF4E0720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model8 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='swish', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(128, activation='swish'),\n",
    "    tf.keras.layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model8.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train model\n",
    "history = model8.fit(X_train, y_train, epochs= 200, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss = model8.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "#predict\n",
    "predictions = model8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec75e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prediction      Actual\n",
      "0    151.573761  119.793729\n",
      "1    233.177032  359.563884\n",
      "2     95.184059  168.934525\n",
      "3    332.961853  355.505368\n",
      "4    520.484131  433.151080\n",
      "..          ...         ...\n",
      "112  571.045654  487.957347\n",
      "113  194.077469  216.123044\n",
      "114  395.130737  425.953734\n",
      "115  292.347015  436.229287\n",
      "116  475.176300  388.424770\n",
      "\n",
      "[117 rows x 2 columns]\n",
      "Mean Absolute Error (MAE): 79.22144891399255\n",
      "Mean Squared Error (MSE): 11131.366104919462\n",
      "Root Mean Squared Error (RMSE): 105.5052894641755\n",
      "R-squared (R²): 0.8029447995748155\n",
      "Mean Absolute Error: 79.22 degrees.\n",
      "Accuracy: 73.35 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ-PC\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if predictions.ndim != 1:\n",
    "    predictions = predictions.flatten()\n",
    "\n",
    "# Check if 'y_test' is not 1-dimensional and reshape if necessary\n",
    "if y_test.ndim != 1:\n",
    "    y_test = y_test.flatten()\n",
    "    \n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "print(results)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(mae, 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b93b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "41       41    0.203042\n",
      "5         5    0.194397\n",
      "28       28    0.193621\n",
      "30       30    0.190869\n",
      "43       43    0.187617\n",
      "34       34    0.183421\n",
      "32       32    0.181464\n",
      "1         1    0.177878\n",
      "26       26    0.176725\n",
      "48       48    0.176262\n",
      "9         9    0.172832\n",
      "36       36    0.169910\n",
      "44       44    0.168361\n",
      "40       40    0.167318\n",
      "38       38    0.167271\n",
      "22       22    0.166036\n",
      "24       24    0.165149\n",
      "49       49    0.164034\n",
      "7         7    0.157641\n",
      "16       16    0.157467\n",
      "20       20    0.154352\n",
      "13       13    0.154066\n",
      "52       52    0.144478\n",
      "18       18    0.143997\n",
      "11       11    0.142230\n",
      "3         3    0.133423\n",
      "51       51    0.130427\n",
      "53       53    0.129799\n",
      "54       54    0.127528\n",
      "50       50    0.123673\n",
      "0         0    0.123278\n",
      "31       31    0.113980\n",
      "6         6    0.113898\n",
      "21       21    0.111020\n",
      "46       46    0.110570\n",
      "39       39    0.110522\n",
      "35       35    0.109868\n",
      "15       15    0.109807\n",
      "12       12    0.108538\n",
      "2         2    0.108333\n",
      "4         4    0.107948\n",
      "23       23    0.107511\n",
      "29       29    0.106541\n",
      "25       25    0.106466\n",
      "10       10    0.105691\n",
      "42       42    0.105234\n",
      "19       19    0.104242\n",
      "27       27    0.104221\n",
      "37       37    0.103713\n",
      "8         8    0.102960\n",
      "56       56    0.102683\n",
      "17       17    0.101558\n",
      "47       47    0.101332\n",
      "55       55    0.100038\n",
      "45       45    0.098060\n",
      "14       14    0.097189\n",
      "33       33    0.092432\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute weights of the first layer\n",
    "weights = model8.layers[0].get_weights()[0]\n",
    "abs_weights = np.abs(weights).mean(axis=1)\n",
    "\n",
    "# Summarize feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': range(X_train.shape[1]), 'Importance': abs_weights})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c3083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
