{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2a9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d432fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>City</th>\n",
       "      <th>Percent Population 25 years and over - Less than 9th grade</th>\n",
       "      <th>Percent Population 25 years and over - 9th to12th (No Diploma)</th>\n",
       "      <th>Percent Population 25 years and over - High School Graduate (and equivalent)</th>\n",
       "      <th>Percent Population 25 years and over - Some college, no degree</th>\n",
       "      <th>Percent Population 25 years and over - Associate's degree</th>\n",
       "      <th>Percent Population 25 years and over - Bachelor's degree</th>\n",
       "      <th>Percent Population 25 years and over - Graduate or Prefessional Degree</th>\n",
       "      <th>Total population</th>\n",
       "      <th>...</th>\n",
       "      <th>Percent 75 to 84 years</th>\n",
       "      <th>Percent 85 years and over</th>\n",
       "      <th>Violent Crimes Sum</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "      <th>Income</th>\n",
       "      <th>House price mean</th>\n",
       "      <th>% All Families</th>\n",
       "      <th>Crime_Rate_per_100k</th>\n",
       "      <th>Percent Home Occupied</th>\n",
       "      <th>Percent Renter Occupied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>73981.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>173.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>28011.0</td>\n",
       "      <td>605675.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>233.843825</td>\n",
       "      <td>46.920853</td>\n",
       "      <td>53.079147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>Alhambra</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>21.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>21.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>83202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28100.0</td>\n",
       "      <td>441616.7</td>\n",
       "      <td>10.7</td>\n",
       "      <td>222.350424</td>\n",
       "      <td>38.053381</td>\n",
       "      <td>61.946619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>15.5</td>\n",
       "      <td>10.7</td>\n",
       "      <td>24.3</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>337259.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1161.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>100404.0</td>\n",
       "      <td>371899.4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>344.245817</td>\n",
       "      <td>48.388510</td>\n",
       "      <td>51.611490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>Antioch</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.3</td>\n",
       "      <td>26.9</td>\n",
       "      <td>31.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>102745.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>864.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>30970.0</td>\n",
       "      <td>181427.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>840.916833</td>\n",
       "      <td>62.767194</td>\n",
       "      <td>37.232806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>Apple Valley</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>33.4</td>\n",
       "      <td>26.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>69387.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>188.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>23900.0</td>\n",
       "      <td>138720.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>270.944125</td>\n",
       "      <td>70.690377</td>\n",
       "      <td>29.309623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year          City  \\\n",
       "0  2010       Alameda   \n",
       "1  2010      Alhambra   \n",
       "2  2010       Anaheim   \n",
       "3  2010       Antioch   \n",
       "4  2010  Apple Valley   \n",
       "\n",
       "   Percent Population 25 years and over - Less than 9th grade  \\\n",
       "0                                                5.5            \n",
       "1                                               10.1            \n",
       "2                                               15.5            \n",
       "3                                                7.7            \n",
       "4                                                1.5            \n",
       "\n",
       "   Percent Population 25 years and over - 9th to12th (No Diploma)  \\\n",
       "0                                                4.5                \n",
       "1                                                7.4                \n",
       "2                                               10.7                \n",
       "3                                                7.3                \n",
       "4                                                7.7                \n",
       "\n",
       "   Percent Population 25 years and over - High School Graduate (and equivalent)  \\\n",
       "0                                               17.0                              \n",
       "1                                               21.7                              \n",
       "2                                               24.3                              \n",
       "3                                               26.9                              \n",
       "4                                               33.4                              \n",
       "\n",
       "   Percent Population 25 years and over - Some college, no degree  \\\n",
       "0                                               19.9                \n",
       "1                                               19.6                \n",
       "2                                               18.8                \n",
       "3                                               31.7                \n",
       "4                                               26.9                \n",
       "\n",
       "   Percent Population 25 years and over - Associate's degree  \\\n",
       "0                                                5.8           \n",
       "1                                                8.9           \n",
       "2                                                6.7           \n",
       "3                                                8.8           \n",
       "4                                               11.0           \n",
       "\n",
       "   Percent Population 25 years and over - Bachelor's degree  \\\n",
       "0                                               31.0          \n",
       "1                                               21.7          \n",
       "2                                               18.0          \n",
       "3                                               13.6          \n",
       "4                                               10.5          \n",
       "\n",
       "   Percent Population 25 years and over - Graduate or Prefessional Degree  \\\n",
       "0                                               16.3                        \n",
       "1                                               10.5                        \n",
       "2                                                6.0                        \n",
       "3                                                4.1                        \n",
       "4                                                9.0                        \n",
       "\n",
       "   Total population  ...  Percent 75 to 84 years  Percent 85 years and over  \\\n",
       "0           73981.0  ...                     3.8                        2.1   \n",
       "1           83202.0  ...                     4.2                        3.0   \n",
       "2          337259.0  ...                     3.0                        1.2   \n",
       "3          102745.0  ...                     2.4                        1.1   \n",
       "4           69387.0  ...                     6.4                        2.2   \n",
       "\n",
       "   Violent Crimes Sum  Unemployment Rate    Income  House price mean  \\\n",
       "0               173.0               11.1   28011.0          605675.9   \n",
       "1               185.0                9.2   28100.0          441616.7   \n",
       "2              1161.0               14.4  100404.0          371899.4   \n",
       "3               864.0               15.3   30970.0          181427.3   \n",
       "4               188.0               12.3   23900.0          138720.2   \n",
       "\n",
       "   % All Families  Crime_Rate_per_100k  Percent Home Occupied  \\\n",
       "0             9.6           233.843825              46.920853   \n",
       "1            10.7           222.350424              38.053381   \n",
       "2            12.4           344.245817              48.388510   \n",
       "3             8.0           840.916833              62.767194   \n",
       "4            10.4           270.944125              70.690377   \n",
       "\n",
       "   Percent Renter Occupied  \n",
       "0                53.079147  \n",
       "1                61.946619  \n",
       "2                51.611490  \n",
       "3                37.232806  \n",
       "4                29.309623  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our input dataset\n",
    "crime_df = pd.read_csv('All_data_cleaned.csv')\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37dde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove crime and city target from features data\n",
    "X = crime_df.drop(columns=['Crime_Rate_per_100k', 'City'])\n",
    "y = crime_df['Crime_Rate_per_100k']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a450ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lxlaz\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 226816.9375 - val_loss: 227398.8281\n",
      "Epoch 2/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 226640.8906 - val_loss: 223702.4531\n",
      "Epoch 3/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 230991.4062 - val_loss: 216320.2656\n",
      "Epoch 4/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 204991.2500 - val_loss: 202698.0000\n",
      "Epoch 5/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 182160.5312 - val_loss: 180337.1875\n",
      "Epoch 6/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 180724.3281 - val_loss: 150888.2188\n",
      "Epoch 7/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 144142.6562 - val_loss: 119313.7500\n",
      "Epoch 8/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108043.4688 - val_loss: 92731.6172\n",
      "Epoch 9/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98201.7500 - val_loss: 75641.9297\n",
      "Epoch 10/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80783.7109 - val_loss: 67215.6641\n",
      "Epoch 11/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71696.8281 - val_loss: 62904.3711\n",
      "Epoch 12/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70167.2969 - val_loss: 59799.7617\n",
      "Epoch 13/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58845.5938 - val_loss: 57179.2461\n",
      "Epoch 14/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68950.4688 - val_loss: 55213.6133\n",
      "Epoch 15/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60975.9258 - val_loss: 53360.7812\n",
      "Epoch 16/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50977.1328 - val_loss: 51943.8164\n",
      "Epoch 17/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44919.5703 - val_loss: 50767.2812\n",
      "Epoch 18/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48600.2344 - val_loss: 49918.5430\n",
      "Epoch 19/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42279.3438 - val_loss: 49081.0117\n",
      "Epoch 20/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47941.8242 - val_loss: 48315.0781\n",
      "Epoch 21/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47995.3008 - val_loss: 47867.0742\n",
      "Epoch 22/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39604.7891 - val_loss: 47457.2812\n",
      "Epoch 23/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46214.3906 - val_loss: 47058.8750\n",
      "Epoch 24/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39449.9531 - val_loss: 46433.8086\n",
      "Epoch 25/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36851.4297 - val_loss: 46225.9727\n",
      "Epoch 26/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40879.6094 - val_loss: 45948.4414\n",
      "Epoch 27/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42192.1992 - val_loss: 45430.4062\n",
      "Epoch 28/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36831.1719 - val_loss: 44986.9648\n",
      "Epoch 29/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36846.7422 - val_loss: 44850.1602\n",
      "Epoch 30/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41099.6719 - val_loss: 44420.4531\n",
      "Epoch 31/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39893.5508 - val_loss: 44023.0664\n",
      "Epoch 32/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38651.8867 - val_loss: 43843.2617\n",
      "Epoch 33/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35470.8203 - val_loss: 43648.2070\n",
      "Epoch 34/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36306.5547 - val_loss: 43301.4961\n",
      "Epoch 35/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40151.9062 - val_loss: 42940.1641\n",
      "Epoch 36/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35116.9141 - val_loss: 42689.7773\n",
      "Epoch 37/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41054.8633 - val_loss: 42425.1836\n",
      "Epoch 38/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40635.0547 - val_loss: 42107.2383\n",
      "Epoch 39/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34895.1094 - val_loss: 41643.9883\n",
      "Epoch 40/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33686.0664 - val_loss: 41566.7344\n",
      "Epoch 41/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34749.8828 - val_loss: 41170.3711\n",
      "Epoch 42/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31801.2949 - val_loss: 41026.8906\n",
      "Epoch 43/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37110.2734 - val_loss: 40603.9648\n",
      "Epoch 44/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30109.6309 - val_loss: 40580.8711\n",
      "Epoch 45/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36402.6680 - val_loss: 40303.8789\n",
      "Epoch 46/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35188.7578 - val_loss: 39868.3555\n",
      "Epoch 47/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30472.1719 - val_loss: 39675.6836\n",
      "Epoch 48/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33370.6094 - val_loss: 39418.9883\n",
      "Epoch 49/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33205.9453 - val_loss: 39235.5430\n",
      "Epoch 50/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29433.7988 - val_loss: 38832.0742\n",
      "Epoch 51/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31125.8145 - val_loss: 38678.9727\n",
      "Epoch 52/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26031.0801 - val_loss: 38351.0977\n",
      "Epoch 53/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30459.6934 - val_loss: 38244.7148\n",
      "Epoch 54/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28128.1152 - val_loss: 38113.8906\n",
      "Epoch 55/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29047.9023 - val_loss: 37678.1758\n",
      "Epoch 56/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31197.7637 - val_loss: 37480.0742\n",
      "Epoch 57/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27813.5293 - val_loss: 37428.8867\n",
      "Epoch 58/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25519.8164 - val_loss: 37321.2148\n",
      "Epoch 59/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32810.6211 - val_loss: 36946.0078\n",
      "Epoch 60/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32433.5117 - val_loss: 36813.6641\n",
      "Epoch 61/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28354.7891 - val_loss: 36600.8398\n",
      "Epoch 62/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27532.2539 - val_loss: 36465.4688\n",
      "Epoch 63/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27437.0176 - val_loss: 36212.3633\n",
      "Epoch 64/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26079.0664 - val_loss: 36054.6211\n",
      "Epoch 65/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26558.0156 - val_loss: 35899.6875\n",
      "Epoch 66/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26953.7090 - val_loss: 35549.3359\n",
      "Epoch 67/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25662.6465 - val_loss: 35540.0742\n",
      "Epoch 68/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24880.9160 - val_loss: 35194.5586\n",
      "Epoch 69/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28886.2441 - val_loss: 35196.2461\n",
      "Epoch 70/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26964.5508 - val_loss: 34902.3906\n",
      "Epoch 71/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27966.4199 - val_loss: 34727.9336\n",
      "Epoch 72/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27096.7988 - val_loss: 34574.0742\n",
      "Epoch 73/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23100.9141 - val_loss: 34406.0430\n",
      "Epoch 74/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23953.8594 - val_loss: 34114.8438\n",
      "Epoch 75/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22686.7812 - val_loss: 34222.5781\n",
      "Epoch 76/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27322.7949 - val_loss: 33974.8789\n",
      "Epoch 77/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24603.0879 - val_loss: 33829.6602\n",
      "Epoch 78/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26597.7832 - val_loss: 33746.7305\n",
      "Epoch 79/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23465.3477 - val_loss: 33554.0508\n",
      "Epoch 80/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23669.6348 - val_loss: 33294.7656\n",
      "Epoch 81/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22284.1211 - val_loss: 33085.3281\n",
      "Epoch 82/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23407.2051 - val_loss: 32850.0469\n",
      "Epoch 83/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24875.2402 - val_loss: 32720.4531\n",
      "Epoch 84/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21494.6562 - val_loss: 32555.6426\n",
      "Epoch 85/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23774.7715 - val_loss: 32488.8145\n",
      "Epoch 86/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22376.4473 - val_loss: 32191.6484\n",
      "Epoch 87/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21936.0078 - val_loss: 32039.3730\n",
      "Epoch 88/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22921.4941 - val_loss: 31902.9375\n",
      "Epoch 89/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21912.3086 - val_loss: 31827.6738\n",
      "Epoch 90/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21152.4492 - val_loss: 31555.5938\n",
      "Epoch 91/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21641.9727 - val_loss: 31377.6719\n",
      "Epoch 92/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23304.4219 - val_loss: 31356.0371\n",
      "Epoch 93/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20668.3594 - val_loss: 31145.3184\n",
      "Epoch 94/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21902.5156 - val_loss: 31069.5215\n",
      "Epoch 95/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21989.6152 - val_loss: 30804.7637\n",
      "Epoch 96/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18540.1426 - val_loss: 30749.2266\n",
      "Epoch 97/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19591.9805 - val_loss: 30542.3906\n",
      "Epoch 98/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20926.3105 - val_loss: 30379.3066\n",
      "Epoch 99/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21319.7031 - val_loss: 30214.8809\n",
      "Epoch 100/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19296.1934 - val_loss: 30203.2441\n",
      "Epoch 101/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20535.5605 - val_loss: 29921.6660\n",
      "Epoch 102/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17269.6758 - val_loss: 29710.4609\n",
      "Epoch 103/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22200.6660 - val_loss: 29664.5781\n",
      "Epoch 104/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19720.9102 - val_loss: 29542.4004\n",
      "Epoch 105/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20445.1035 - val_loss: 29359.2129\n",
      "Epoch 106/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19475.3438 - val_loss: 29184.2246\n",
      "Epoch 107/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19229.9277 - val_loss: 29180.7754\n",
      "Epoch 108/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18332.1777 - val_loss: 28876.0840\n",
      "Epoch 109/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19502.6934 - val_loss: 28902.9219\n",
      "Epoch 110/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17658.5273 - val_loss: 28741.1484\n",
      "Epoch 111/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17587.6309 - val_loss: 28480.3105\n",
      "Epoch 112/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19161.6055 - val_loss: 28296.5469\n",
      "Epoch 113/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17875.0898 - val_loss: 28218.0879\n",
      "Epoch 114/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18475.9062 - val_loss: 28041.6270\n",
      "Epoch 115/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18706.9434 - val_loss: 27937.7031\n",
      "Epoch 116/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17380.2461 - val_loss: 27743.7031\n",
      "Epoch 117/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15543.2275 - val_loss: 27681.2832\n",
      "Epoch 118/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16732.1992 - val_loss: 27428.3203\n",
      "Epoch 119/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16658.7637 - val_loss: 27378.7324\n",
      "Epoch 120/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15889.5830 - val_loss: 27106.5957\n",
      "Epoch 121/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16528.4746 - val_loss: 27063.7188\n",
      "Epoch 122/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16870.5156 - val_loss: 26833.0762\n",
      "Epoch 123/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16021.3936 - val_loss: 26795.7812\n",
      "Epoch 124/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14290.1309 - val_loss: 26535.1816\n",
      "Epoch 125/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14261.4668 - val_loss: 26586.1875\n",
      "Epoch 126/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18521.2910 - val_loss: 26391.9941\n",
      "Epoch 127/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15398.9072 - val_loss: 26300.0781\n",
      "Epoch 128/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15188.6494 - val_loss: 26049.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14737.4609 - val_loss: 26045.6152\n",
      "Epoch 130/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16450.4902 - val_loss: 25812.7500\n",
      "Epoch 131/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17373.8145 - val_loss: 25740.7969\n",
      "Epoch 132/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14526.7959 - val_loss: 25649.5078\n",
      "Epoch 133/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15474.5176 - val_loss: 25421.6035\n",
      "Epoch 134/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16987.2637 - val_loss: 25336.3887\n",
      "Epoch 135/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14514.6562 - val_loss: 25349.2754\n",
      "Epoch 136/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14487.3125 - val_loss: 25078.0527\n",
      "Epoch 137/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15168.5391 - val_loss: 24816.5254\n",
      "Epoch 138/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14802.6748 - val_loss: 24678.5469\n",
      "Epoch 139/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14183.1562 - val_loss: 24486.9395\n",
      "Epoch 140/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13625.6582 - val_loss: 24417.9160\n",
      "Epoch 141/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13412.6162 - val_loss: 24102.5312\n",
      "Epoch 142/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14051.0596 - val_loss: 24138.9434\n",
      "Epoch 143/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13642.9922 - val_loss: 23974.5723\n",
      "Epoch 144/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13749.1914 - val_loss: 23801.1504\n",
      "Epoch 145/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12684.6924 - val_loss: 23555.1562\n",
      "Epoch 146/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14527.6260 - val_loss: 23663.7031\n",
      "Epoch 147/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12280.1357 - val_loss: 23335.0254\n",
      "Epoch 148/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14836.5732 - val_loss: 23273.2285\n",
      "Epoch 149/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14071.3477 - val_loss: 23149.5371\n",
      "Epoch 150/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12759.3457 - val_loss: 22973.6465\n",
      "Epoch 151/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13862.7598 - val_loss: 22701.9141\n",
      "Epoch 152/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12484.0771 - val_loss: 22635.8594\n",
      "Epoch 153/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13142.8516 - val_loss: 22428.6895\n",
      "Epoch 154/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13686.9502 - val_loss: 22477.7246\n",
      "Epoch 155/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12009.8936 - val_loss: 22334.8594\n",
      "Epoch 156/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12133.6348 - val_loss: 22159.7363\n",
      "Epoch 157/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12243.5225 - val_loss: 22012.9238\n",
      "Epoch 158/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12761.3594 - val_loss: 21913.0840\n",
      "Epoch 159/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12332.8398 - val_loss: 21777.0215\n",
      "Epoch 160/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12523.4141 - val_loss: 21631.6816\n",
      "Epoch 161/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12418.2090 - val_loss: 21486.4766\n",
      "Epoch 162/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13507.1748 - val_loss: 21366.4492\n",
      "Epoch 163/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11505.5527 - val_loss: 21307.2637\n",
      "Epoch 164/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12151.3555 - val_loss: 21136.8340\n",
      "Epoch 165/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11973.4688 - val_loss: 21058.3379\n",
      "Epoch 166/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11857.0342 - val_loss: 20914.2520\n",
      "Epoch 167/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10999.7705 - val_loss: 20810.1660\n",
      "Epoch 168/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10087.4248 - val_loss: 20778.8027\n",
      "Epoch 169/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10236.8818 - val_loss: 20564.0410\n",
      "Epoch 170/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9903.5811 - val_loss: 20437.7676\n",
      "Epoch 171/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10987.1182 - val_loss: 20308.1992\n",
      "Epoch 172/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11372.4746 - val_loss: 20332.4941\n",
      "Epoch 173/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10870.4482 - val_loss: 20104.4941\n",
      "Epoch 174/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10703.4688 - val_loss: 20041.4375\n",
      "Epoch 175/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10149.8408 - val_loss: 19879.9043\n",
      "Epoch 176/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10120.3848 - val_loss: 19760.2109\n",
      "Epoch 177/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10156.4287 - val_loss: 19624.9531\n",
      "Epoch 178/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10073.5000 - val_loss: 19510.3691\n",
      "Epoch 179/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10098.0586 - val_loss: 19473.9453\n",
      "Epoch 180/180\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10323.5566 - val_loss: 19335.6777\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 14356.5879\n",
      "Test Loss: 13522.666015625\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Mean Squared Error: 13948.750958000168\n",
      "R-squared: 0.814841794707898\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # No activation for regression\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train model\n",
    "history = model.fit(X_train, y_train, epochs=180, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "#predict\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "# calculate r2\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d061fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c39a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
